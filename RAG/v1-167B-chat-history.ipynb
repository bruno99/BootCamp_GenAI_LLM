{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c972d437-04ab-4a71-ae2a-bce7a2202a40",
   "metadata": {},
   "source": [
    "# How to Add Memory of Past Questions in a RAG App: Chat Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a72b80e-6107-4330-96c4-42cdd7daaed0",
   "metadata": {},
   "source": [
    "* [Link to the Documentation Page.](https://python.langchain.com/docs/use_cases/question_answering/chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dc6a53-9b07-43a7-9494-76b048e1ec6d",
   "metadata": {},
   "source": [
    "## Goal\n",
    "* Allow the user to have a back-and-forth conversation, meaning the application needs some sort of “memory” of past questions and answers.\n",
    "* In this guide we focus on adding logic for incorporating historical messages, and NOT on chat history management. Chat history management is covered [here](https://python.langchain.com/docs/expression_language/how_to/message_history)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4735fdb-05c3-49bd-957b-e839815abb77",
   "metadata": {},
   "source": [
    "## Dependencies and Necessary Modules\n",
    "* Same as with the project we develop for the Quickstart Guide (see notebook 159-rag-quickstart)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119acd70-f6fa-44ee-a1ec-ba94557c3990",
   "metadata": {},
   "source": [
    "## .env file\n",
    "* OpenAI API key.\n",
    "* LangSmith credentials.\n",
    "* Our LangSmith project name: RAGchatMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e17b1f-1765-4b6c-a1a6-34d95bd8db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6aa724-4242-442e-8741-4ed013dd6e1e",
   "metadata": {},
   "source": [
    "## Open LangSmith to track the following operations\n",
    "* smith.langchain.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30ea29-1a94-4772-99d9-c6bda14f5a31",
   "metadata": {},
   "source": [
    "## The initial RAG App without sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "717ef3ab-96a9-45c8-8dfc-417a7f4ecbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a41c093-5d7b-4f78-8bde-f5d4b0339294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "bs_strainer = bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8dd9628-b480-4709-92f2-c9cfee5b0bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a technique used to break down complex tasks into smaller and simpler steps. This approach helps agents to plan and execute tasks more efficiently by transforming big tasks into manageable components. Task decomposition can be achieved through various methods such as prompting with specific instructions or utilizing human inputs.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47a92e-157b-4978-9d80-3085c18eb94a",
   "metadata": {},
   "source": [
    "## Contextualizing the question: define a sub-chain that takes historical messages and the latest user question\n",
    "* We’ll use a prompt that includes a MessagesPlaceholder variable under the name “chat_history”. This allows us to pass in a list of Messages to the prompt using the “chat_history” input key, and these messages will be inserted after the system message and before the human message containing the latest question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22624856-7f28-4fe7-b38d-da24e05f8e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dcc538-1c6a-4848-b699-2aedf1b4071b",
   "metadata": {},
   "source": [
    "Now we can ask follow-up questions that reference past messages and have them reformulated into standalone questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe924f4-27a5-4fad-a0de-a1b906afaa16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the definition of \"large\" in this context?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "contextualize_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"What is meant by large\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f47d68f-64a0-4deb-a243-ab1c162c2f07",
   "metadata": {},
   "source": [
    "## Chain with chat history\n",
    "* We will add some routing functionality to only run the “condense question chain” when our chat history isn’t empty.\n",
    "* We will take advantage of the fact that if a function in an LCEL (LangChain Expression Language) chain returns another chain, that chain will itself be invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51a01a8f-d1e6-4ba0-aa5d-638e4e16598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04fb0d20-f0b7-4635-9c0c-2ca1fc4e4845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Task decomposition can be done in common ways such as using Language Model (LLM) with simple prompting, task-specific instructions, or human inputs. LLMs can be prompted with specific instructions like \"Steps for XYZ\" to break down tasks into smaller steps. Task-specific instructions, such as \"Write a story outline,\" can also help in decomposing tasks into manageable subgoals. Additionally, human inputs can be utilized to decompose complex tasks effectively.', response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 640, 'total_tokens': 730}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd9233-7ff6-4c95-84aa-ca2f376f265a",
   "metadata": {},
   "source": [
    "## Advanced technique\n",
    "* In a real Q&A application we’ll want some way of persisting chat history and some way of automatically inserting and updating it. You can learn about how to do it [here](https://python.langchain.com/docs/expression_language/how_to/message_history)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe48a3c-e18f-48c5-b461-da4355db66dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
