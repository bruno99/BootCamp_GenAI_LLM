{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a43691f4-b9da-4c8c-b303-f4bfaeffd2d5",
   "metadata": {},
   "source": [
    "# Part 5: Advanced techniques\n",
    "* multiquery technique\n",
    "* chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72e503-f93a-4131-a8d2-1c585b8a2019",
   "metadata": {},
   "source": [
    "## IMPORTANT: Installation with the exact packages we used\n",
    "* When you download a full stack app you need to make sure that both backend and frontend use the original packages in order to avoid potential errors caused by installing more modern versions of these packages.\n",
    "* Since we used pip to install the original backend packages and froze them using pip freeze, you will now use \"pip install -r requirements.txt\" to install them. Since we also used poetry, you will also use \"poetry install\".\n",
    "* Since we used npx to install the original frontend packages, you will now use \"npm ci\" to install them.\n",
    "#### Download the code\n",
    "* Download the code from the github repository.\n",
    "#### Backend installation\n",
    "* Since we used both pyenv and poetry to build this project, you will have to use the following approach to install the backend.\n",
    "* In the terminal, make sure you are in the root directory of the project (v1-166-part5). Pay attention: the root directory of the project and the backend directory have an identic name. Do not mistake them, be sure you are in the root directory of the project now.\n",
    "* **Create a virtual environment and use pip install to make sure you install the exact same packages we used**:\n",
    "    * pyenv virtualenv 3.11.4 your-virtual-environment-name\n",
    "    * pyenv activate your-virtual-environment-name\n",
    "    * pip install -r requirements.txt\n",
    "* **Go to the backend directory, create a virtual environment and use poetry install to make sure you install the exact same packages we used**:\n",
    "    * cd v1-166-part5\n",
    "    * poetry install\n",
    "#### Frontend installation\n",
    "* Open a second terminal window, make sure you are in the root directory of the project (v1-166-part5). Pay attention: the root directory of the project and the backend directory have an identic name. Do not mistake them, be sure you are in the root directory of the project now.\n",
    "* **Go to the frontend directory, and use npm ci to make sure you install the exact same packages we used**:\n",
    "    * cd frontend\n",
    "    * npm ci\n",
    "#### Ready to go!\n",
    "* You can now see the code of the app in Visual Studio Code.\n",
    "* Relax and review the following steps. Remember, since you have pre-installed the modules you will not have to re-install them again.\n",
    "* **IMPORTANT**: due to the changes we have done in rag_chain.py, if you try to run the backend now you will see an error message. For this lecture, just run the frontend and wait until the following lecture to run the backend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf78548-7880-4230-ab98-6885c8671b94",
   "metadata": {},
   "source": [
    "# Multiquery technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e032ecf-3661-41b8-9bc4-c589001a6a87",
   "metadata": {},
   "source": [
    "* First, let's update the final_chain in app/rag_chain.py. We will go from this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf0c53b-3848-457d-bcc5-1b31fc07642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_chain = (\n",
    "#         RunnableParallel(\n",
    "#             context=(itemgetter(\"question\") | vector_store.as_retriever()),\n",
    "#             question=itemgetter(\"question\")\n",
    "#         ) |\n",
    "#         RunnableParallel(\n",
    "#             answer=(ANSWER_PROMPT | llm),\n",
    "#             docs=itemgetter(\"context\")\n",
    "#         )\n",
    "# ).with_types(input_type=RagInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5fe0c-41ac-47e7-922e-b017cddc79ee",
   "metadata": {},
   "source": [
    "* To this (see the change in line 3: `multiquery`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06cb58f-0335-404e-b176-b9e82f63e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_chain = (\n",
    "#         RunnableParallel(\n",
    "#             context=(itemgetter(\"question\") | multiquery),\n",
    "#             question=itemgetter(\"question\")\n",
    "#         ) |\n",
    "#         RunnableParallel(\n",
    "#             answer=(ANSWER_PROMPT | llm),\n",
    "#             docs=itemgetter(\"context\")\n",
    "#         )\n",
    "# ).with_types(input_type=RagInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c39f9-e5ee-4b03-b75d-80b7996dbf83",
   "metadata": {},
   "source": [
    "* Now we need to define this new multiquery variable. Before the final_chain, we will add:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d8b31f-4ad6-46eb-b2d2-099416d0cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiquery = MultiQueryRetriever.from_llm(\n",
    "#     retriever=vector_store.as_retriever(),\n",
    "#     llm=llm,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb86ca-f02b-4b2a-a0c9-d88d2b8033e8",
   "metadata": {},
   "source": [
    "* To be able to use [MultiQueryRetriever](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever/), we will have to add this import at the top of the file:\n",
    "* `from langchain.retrievers.multi_query import MultiQueryRetriever`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b09d0f-b204-4177-a788-8aa53608ccf9",
   "metadata": {},
   "source": [
    "#### We can now check this asking a question to our RAG app and tracking the response on LangSmith."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f026da31-7246-4704-b842-5f393c601515",
   "metadata": {},
   "source": [
    "* Before doing it, we are going to include a small modification in frontend/src/App.tsx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2c865-2116-44a8-b845-49a7ce99f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# const handleSendMessage = async (message: string) => {\n",
    "#     setInputValue(\"\")\n",
    "\n",
    "#     setMessages(prevMessages => [...prevMessages, {message, isUser: true}]);\n",
    "\n",
    "#     await fetchEventSource(`http://localhost:8000/rag/stream`, {\n",
    "#       method: 'POST',\n",
    "#       openWhenHidden: true,\n",
    "#       headers: {\n",
    "#         'Content-Type': 'application/json',\n",
    "#       },\n",
    "#       body: JSON.stringify({\n",
    "#         input: {\n",
    "#           question: message,\n",
    "#         }\n",
    "#       }),\n",
    "#       onmessage(event) {\n",
    "#         if (event.event === \"data\") {\n",
    "#           handleReceiveMessage(event.data);\n",
    "#         }\n",
    "#       },\n",
    "#     })\n",
    "#   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e41bc43-a581-4c18-bd7b-6cb752161c61",
   "metadata": {},
   "source": [
    "We added `openWhenHidden: true`. Let's explain what this does in simple terms.\n",
    "* Imagine you're watching a live sports game on your phone but then switch to another app to send a quick message. Normally, your phone might pause the game to save battery and data because you're not directly looking at it. But if there was a special rule saying \"Keep showing the game even if I'm not currently watching,\" your phone would continue to play the game in the background, so you don't miss anything important.\n",
    "* `openWhenHidden: true` is like that special rule. It tells the website, \"Keep listening for updates from the server even if I've moved to a different tab or minimized the browser.\" This way, the website can keep getting new messages or updates automatically, without you having to keep it open and visible all the time.\n",
    "* In short, this small change will allow us to open LangSmith while our App is in the middle of delivering a response without disrupting this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c15be6-8203-457e-9076-e804be850c50",
   "metadata": {},
   "source": [
    "#### Now, if you ask a question to the RAG App and track it with LangSmith, you will see the Multiquery technique at work.\n",
    "* If you open the trace in LangSmith you will see that the Retriever step has now 3 other Retriever steps. If you look at the ChatOpenAI step, you will see how the LLM has generated the 3 multi-queries that help the app to find the best answer possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59776ec8-c277-4af4-99ac-79d53d8e5eb6",
   "metadata": {},
   "source": [
    "# Chat history\n",
    "* We will use the [RunnableWithMesageHistory](https://python.langchain.com/docs/expression_language/how_to/message_history/) from Langchain to update app/rag_chain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a587cbc7-f3b3-43fa-9210-825729987dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_chain = (\n",
    "#         RunnableParallel(\n",
    "#             context=(itemgetter(\"question\") | multiquery),\n",
    "#             question=itemgetter(\"question\")\n",
    "#         ) |\n",
    "#         RunnableParallel(\n",
    "#             answer=(ANSWER_PROMPT | llm),\n",
    "#             docs=itemgetter(\"context\")\n",
    "#         )\n",
    "# ).with_types(input_type=RagInput)\n",
    "\n",
    "# postgres_memory_url = \"postgresql+psycopg://postgres:postgres@localhost:5432/pdf_rag_history\"\n",
    "\n",
    "# get_session_history = lambda session_id: SQLChatMessageHistory(\n",
    "#     connection_string=postgres_memory_url,\n",
    "#     session_id=session_id\n",
    "# )\n",
    "\n",
    "# template_with_history=\"\"\"\n",
    "# Given the following conversation and a follow\n",
    "# up question, rephrase the follow up question\n",
    "# to be a standalone question, in its original\n",
    "# language\n",
    "\n",
    "# Chat History:\n",
    "# {chat_history}\n",
    "# Follow Up Input: {question}\n",
    "# Standalone question:\"\"\"\n",
    "\n",
    "# standalone_question_prompt = PromptTemplate.from_template(template_with_history)\n",
    "\n",
    "# standalone_question_mini_chain = RunnableParallel(\n",
    "#     question=RunnableParallel(\n",
    "#         question=RunnablePassthrough(),\n",
    "#         chat_history=lambda x:get_buffer_string(x[\"chat_history\"])\n",
    "#     )\n",
    "#     | standalone_question_prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "\n",
    "# final_chain = RunnableWithMessageHistory(\n",
    "#     runnable=standalone_question_mini_chain | old_chain,\n",
    "#     input_messages_key=\"question\",\n",
    "#     history_messages_key=\"chat_history\",\n",
    "#     output_messages_key=\"answer\",\n",
    "#     get_session_history=get_session_history,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1713eb-2d02-45bb-84dd-1dae0a56986d",
   "metadata": {},
   "source": [
    "* See the new imports we have added at the top of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e18dcc-650b-46fd-a86c-f2ee505f7ebb",
   "metadata": {},
   "source": [
    "The previous code outlines a pipeline to handle chat interactions, specifically for transforming follow-up questions into standalone questions and providing answers based on historical context. Here’s a breakdown of the key components and their roles:\n",
    "\n",
    "**Processing Chains**\n",
    "\n",
    "1. **`old_chain`**:\n",
    "   - Uses `RunnableParallel` to process inputs in parallel.\n",
    "   - Extracts the \"question\" directly from the input and retrieves related context using `multiquery`.\n",
    "   - Another parallel operation generates an \"answer\" using a language model (`llm`) prompted by `ANSWER_PROMPT` and retrieves document (\"docs\") context.\n",
    "   - The chain is typed with `RagInput`, expecting an input format where \"question\" is a key.\n",
    "\n",
    "2. **`postgres_memory_url`**:\n",
    "   - Defines a connection string to a PostgreSQL database that stores chat histories.\n",
    "\n",
    "3. **`get_session_history`**:\n",
    "   - A lambda function creating an instance of `SQLChatMessageHistory` for a given session. This allows fetching chat history from the PostgreSQL database.\n",
    "\n",
    "4. **`template_with_history`**:\n",
    "   - A multi-line string defining a template to rephrase follow-up questions into standalone questions, including chat history for context.\n",
    "\n",
    "5. **`standalone_question_prompt`**:\n",
    "   - Converts `template_with_history` into a `PromptTemplate`, preparing it for use with a language model.\n",
    "\n",
    "6. **`standalone_question_mini_chain`**:\n",
    "   - Constructs a mini processing chain that combines the follow-up question and chat history, uses `standalone_question_prompt` to generate a prompt, feeds it through the language model (`llm`), and parses the string output.\n",
    "\n",
    "**Final Processing Pipeline**\n",
    "\n",
    "- **`final_chain`**:\n",
    "  - Combines `standalone_question_mini_chain` with `old_chain`, applying `RunnableWithMessageHistory`.\n",
    "  - This setup enables the system to process a user's question, factor in historical context, generate a standalone question, and then produce an answer.\n",
    "  - It manages input and output keys for questions, chat history, and answers, ensuring the correct flow of data through the system.\n",
    "  - Utilizes `get_session_history` to fetch and incorporate session-based chat history, enriching the context for generating responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bee7ab-ddde-4fd5-9f1d-35e460492a44",
   "metadata": {},
   "source": [
    "Let's explain it in simple terms. Imagine you're having a conversation with a really smart robot that can remember everything you've talked about. You're asking questions, and it's answering them, but sometimes you ask a follow-up question that doesn't make much sense on its own. So, the robot has to figure out how to turn that follow-up question into a new, clear question that anyone could understand, even if they didn't know what you were talking about before. Plus, it has to remember all your past questions and answers to give you the best reply. Here’s how the code does this:\n",
    "\n",
    "1. **The `old_chain` Part**: Think of this as the robot's basic way of understanding your question and finding the best answer from a big book of knowledge it has. It looks at your question, digs up some related stuff, and then uses that to come up with an answer.\n",
    "\n",
    "2. **Remembering Past Conversations**: There's a part where the robot is told how to remember everything you've talked about. It uses a special notebook stored somewhere safe (like a database) to jot down all the questions and answers from your conversation.\n",
    "\n",
    "3. **Turning Follow-up Questions into Standalone Questions**: The robot uses a special format to think about how to change your follow-up question into a new, standalone question. This is important because it wants to make sure the question makes sense on its own.\n",
    "\n",
    "4. **The Mini-Chain for Standalone Questions**: This is a small set of steps the robot takes to actually turn the follow-up question into a standalone question. It looks at the history of your chat, the question you asked, and then rewrites it to be clear and understandable by itself.\n",
    "\n",
    "5. **The Final Chain**: Finally, all of this comes together. The robot first tries to turn your follow-up question into a standalone question and then goes through its basic steps of understanding and answering the question, remembering everything you've talked about before. This way, it can give you a really good answer.\n",
    "\n",
    "In simple terms, this code is like instructions for a very smart robot that helps it understand and remember your conversation, and make sure every question you ask is clear and makes sense on its own, even if it's a follow-up question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734ca07-ebc8-4e94-bc0c-83a91523434f",
   "metadata": {},
   "source": [
    "## The frontend needs to send the session_id. \n",
    "* In order to do that, let's update frontend/src/App.tsx."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452d8869-755c-4b5a-acf0-b7ce2f76491e",
   "metadata": {},
   "source": [
    "* First, you will need to install this module via terminal:\n",
    "* `npm install --save-dev @types/uuid`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119158eb-f309-465b-95b5-36b4d0034df4",
   "metadata": {},
   "source": [
    "* Then, you can update frontend/src/App.tsx like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c418a4-19aa-4803-986f-7c216e06377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import {v4 as uuidv4} from 'uuid';\n",
    "\n",
    "\n",
    "# #Inside Function App()\n",
    "#   const sessionIdRef = useRef<string>(uuidv4());\n",
    "\n",
    "#   useEffect(() => {\n",
    "#     sessionIdRef.current = uuidv4();\n",
    "#   }, []);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a87e2-f4ee-49da-8e0e-5a10dd7be372",
   "metadata": {},
   "source": [
    "The previous code shows the use of the `uuid` library to generate a unique identifier (UUID) and React hooks (`useRef` and `useEffect`) within a functional component named `App()`. Let's break down what each part does:\n",
    "\n",
    "1. **Importing `uuidv4`**: The `uuid` library provides various methods to generate unique identifiers. `v4` refers to version 4 of the UUID algorithm, which generates random UUIDs. The statement `import {v4 as uuidv4} from 'uuid';` imports this specific method and renames it to `uuidv4` for use within your component.\n",
    "\n",
    "2. **`useRef` Hook**: `useRef` is a hook that allows you to persist values across re-renders without causing additional renders when its value changes. Here, it's used to create a reference (`sessionIdRef`) that holds a string value. Initially, this reference is set to a new UUID generated by `uuidv4()`. The purpose of using `useRef` here is to have a consistent way to access a value (the UUID) that doesn't trigger re-renders when it changes.\n",
    "\n",
    "3. **`useEffect` Hook**: `useEffect` lets you perform side effects in functional components. It's akin to lifecycle methods in class components like `componentDidMount`, `componentDidUpdate`, and `componentWillUnmount`. The provided effect function is run after the component mounts (and after every update, depending on the dependencies array). In this case, the effect:\n",
    "   - Runs only once after the initial render, because the dependencies array (`deps:[]`) is empty. This behavior mimics `componentDidMount` in class components.\n",
    "   - Inside the effect, it assigns a new UUID to `sessionIdRef.current`, effectively overwriting the initial value assigned during the component's initialization with `useRef`.\n",
    "\n",
    "#### What Does This Code Do?\n",
    "\n",
    "When the component `App()` first renders:\n",
    "- It creates a `sessionIdRef` reference with an initial UUID value.\n",
    "\n",
    "Then, right after mounting:\n",
    "- The `useEffect` hook runs and assigns a new UUID to `sessionIdRef.current`.\n",
    "\n",
    "The practical effect of this code is that it ensures `sessionIdRef` has a unique, stable UUID value that can be accessed throughout the component's lifecycle without causing re-renders. The UUID is refreshed only once (due to `useEffect` with an empty dependencies array) after the component mounts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6365e679-c2be-49dc-a592-0d807ff3778e",
   "metadata": {},
   "source": [
    "Let's explain this in simple terms. Imagine you're at a huge party where everyone needs a unique sticker to enter. When you first decide to go, you make yourself a sticker at home using a special sticker machine (this is like generating the first unique ID with `uuidv4()` when the component is first used).\n",
    "\n",
    "Now, when you actually step into the party, you notice there's another sticker machine right at the entrance. Even though you already have a sticker, you use this machine to get a new one just because it's there (this is like using `useEffect` to generate a new unique ID right after the component loads for the first time).\n",
    "\n",
    "So, in simple terms, this code does two main things in our party analogy:\n",
    "1. **Before going to the party**: It creates a unique sticker for you at home.\n",
    "2. **As soon as you arrive at the party**: It gives you a new unique sticker, replacing the one you made at home, but after that, it doesn't give you more stickers no matter how many times you go out and come back in.\n",
    "\n",
    "In your component:\n",
    "- **At the start (with `useRef`)**: It makes a unique code (like a special sticker) and remembers it.\n",
    "- **Right after the component appears on the screen (with `useEffect`)**: It makes another new unique code and updates the remembered code with this new one. But, it only does this once, right after the component first shows up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1e9a4-4111-47e0-86fe-897448986ee2",
   "metadata": {},
   "source": [
    "## Now we need to update the handleSendMessage function\n",
    "* See the changes after `config: {`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505bc6d-8818-4b01-a209-0aca39c4d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# const handleSendMessage = async (message: string) => {\n",
    "#     setInputValue(\"\")\n",
    "\n",
    "#     setMessages(prevMessages => [...prevMessages, {message, isUser: true}]);\n",
    "\n",
    "#     await fetchEventSource(`http://localhost:8000/rag/stream`, {\n",
    "#       method: 'POST',\n",
    "#       openWhenHidden: true,\n",
    "#       headers: {\n",
    "#         'Content-Type': 'application/json',\n",
    "#       },\n",
    "#       body: JSON.stringify({\n",
    "#         input: {\n",
    "#           question: message,\n",
    "#         },\n",
    "#         config: {\n",
    "#           configurable: {\n",
    "#             sessionId: sessionIdRef.current\n",
    "#           }\n",
    "#         }\n",
    "#       }),\n",
    "#       onmessage(event) {\n",
    "#         if (event.event === \"data\") {\n",
    "#           handleReceiveMessage(event.data);\n",
    "#         }\n",
    "#       },\n",
    "#     })\n",
    "#   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec5be7-8a0d-4868-9cc6-f05468ebd181",
   "metadata": {},
   "source": [
    "## Now we need to create the postgres database to store the chat history\n",
    "* In terminal:\n",
    "    * `psql -U postgres`\n",
    "    * `CREATE DATABASE pdf_rag_history`\n",
    "    * `\\q`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f0483-5ba2-48fa-a2ca-a6f87292dd22",
   "metadata": {},
   "source": [
    "## You can now ask one question in the App, and then a follow up question.\n",
    "* The key here is that the follow up question refers to the first question. The App will need to know the chat history to be able to answer the follow up question properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0809518c-72b1-4c37-984d-763d5b48ad98",
   "metadata": {},
   "source": [
    "## Check the traces in LangSmith\n",
    "* See how now the chat history appears at the beginning of the trace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84674ccd-2056-4cf6-85e8-740643474950",
   "metadata": {},
   "source": [
    "## Check how the chat history is being stored in the database\n",
    "In your terminal:\n",
    "* `psql -U postgres`\n",
    "* `\\c pdf_rag_history`\n",
    "* `SELECT * FROM public.message_store;`\n",
    "* `\\q`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae57a2b8-f174-4cbd-a883-f8383d8ae3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
