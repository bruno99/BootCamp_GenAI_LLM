{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c972d437-04ab-4a71-ae2a-bce7a2202a40",
   "metadata": {},
   "source": [
    "# How to Get the Source Documents used to produce a RAG answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a72b80e-6107-4330-96c4-42cdd7daaed0",
   "metadata": {},
   "source": [
    "* [Link to the Documentation Page.](https://python.langchain.com/docs/use_cases/question_answering/sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dc6a53-9b07-43a7-9494-76b048e1ec6d",
   "metadata": {},
   "source": [
    "## Goal\n",
    "* Show user the sources that were used to generate the RAG answer returning the Documents that were retrieved in each generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4735fdb-05c3-49bd-957b-e839815abb77",
   "metadata": {},
   "source": [
    "## Dependencies and Necessary Modules\n",
    "* Same as with the project we develop for the Quickstart Guide (see notebook 159-rag-quickstart)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119acd70-f6fa-44ee-a1ec-ba94557c3990",
   "metadata": {},
   "source": [
    "## .env file\n",
    "* OpenAI API key.\n",
    "* LangSmith credentials.\n",
    "* Our LangSmith project name: RAGgetSources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60e17b1f-1765-4b6c-a1a6-34d95bd8db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6aa724-4242-442e-8741-4ed013dd6e1e",
   "metadata": {},
   "source": [
    "## Open LangSmith to track the following operations\n",
    "* smith.langchain.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30ea29-1a94-4772-99d9-c6bda14f5a31",
   "metadata": {},
   "source": [
    "## The initial RAG App without sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717ef3ab-96a9-45c8-8dfc-417a7f4ecbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a41c093-5d7b-4f78-8bde-f5d4b0339294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "bs_strainer = bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8dd9628-b480-4709-92f2-c9cfee5b0bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a technique used to break down complex tasks into smaller and simpler steps. This approach helps agents plan and execute tasks more efficiently by dividing them into manageable subtasks. Task decomposition can be achieved through various methods, including using prompting techniques, task-specific instructions, or human inputs.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47a92e-157b-4978-9d80-3085c18eb94a",
   "metadata": {},
   "source": [
    "## Adding sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22624856-7f28-4fe7-b38d-da24e05f8e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "\n",
    "response = rag_chain_with_source.invoke(\"What is Task Decomposition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32bee57e-fed2-46cf-a001-2a7b1e579e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Task Decomposition\n"
     ]
    }
   ],
   "source": [
    "print(f\"Question: {response['question']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4de1caac-cf3e-410b-bf1a-6c4cdd8d96c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Task decomposition is a technique used to break down complex tasks into smaller and simpler steps, making it easier for agents to plan and execute them. It involves transforming big tasks into manageable ones by thinking step by step or exploring multiple reasoning possibilities at each step. This process can be done using prompting techniques, task-specific instructions, or human inputs.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer: {response['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0baa66e2-f11b-49ae-b59f-9001e9479e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1 Content:\n",
      "Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Document 1 Metadata:\n",
      "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "\n",
      "Document 2 Content:\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Document 2 Metadata:\n",
      "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "\n",
      "Document 3 Content:\n",
      "The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\n",
      "Document 3 Metadata:\n",
      "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "\n",
      "Document 4 Content:\n",
      "Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\n",
      "The system comprises of 4 stages:\n",
      "(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\n",
      "Instruction:\n",
      "Document 4 Metadata:\n",
      "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the context list and print each document's content and metadata\n",
    "for index, document in enumerate(response['context']):\n",
    "    print(f\"\\nDocument {index + 1} Content:\\n{document.page_content}\")\n",
    "    print(f\"Document {index + 1} Metadata:\\n{document.metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6607f5-3b36-44ae-a721-2166eb2c6537",
   "metadata": {},
   "source": [
    "## Aditional techniques to extract Citations in RAG Apps\n",
    "* [Link to the Documentation Page](https://python.langchain.com/docs/use_cases/question_answering/citations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fd28ab-08da-4aa5-8040-54a4c4195515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
