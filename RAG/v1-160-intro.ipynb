{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "094a2cba-46a7-4a20-9a9a-71647782ecec",
   "metadata": {},
   "source": [
    "# RAG with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85a926-3fce-43fd-8998-14527d0ab9a8",
   "metadata": {},
   "source": [
    "## Overview\n",
    "* One of the most powerful applications enabled by LLMs is question-answering chatbots.\n",
    "* These apps can answer questions about specific source information.\n",
    "* They use the RAG technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8012c-b497-41a0-ac2c-84b80c7e0d9f",
   "metadata": {},
   "source": [
    "#### What is RAG?\n",
    "* Augmenting LLM knowledge with additional data.\n",
    "    * Private data.\n",
    "    * Recent data (introduced after the LLM's cutoff date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3312bc2-1935-4c3e-a69b-55226668dd0e",
   "metadata": {},
   "source": [
    "#### LangChain and RAG\n",
    "* LangChain has a number of components designed to help build RAG apps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c7911-07b0-43c6-b5f9-d03ba837a4e5",
   "metadata": {},
   "source": [
    "#### Our focus now: RAG for unstructured data\n",
    "* For LangChain RAG with structured data, see:\n",
    "    * [RAG with SQL data](https://python.langchain.com/docs/use_cases/sql/)\n",
    "    * [RAG with code data](https://python.langchain.com/docs/use_cases/code_understanding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1321be-25a4-4918-a9a2-6ffbea66af55",
   "metadata": {},
   "source": [
    "## RAG Architecture\n",
    "* A typical RAG app has 2 main components, sometimes separated in 3:\n",
    "    * Indexing: load data and index it.\n",
    "    * Retrieval and generation: question and answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3409feb2-77ce-4224-bbbb-cec70027ee7c",
   "metadata": {},
   "source": [
    "#### Steps of the Indexing phase\n",
    "* Load text with a [Document Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/).\n",
    "* Split text into small chunks with a [Splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/).\n",
    "* Convert chunks into [embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding/) and store them in a [vector database, also called vector store](https://python.langchain.com/docs/modules/data_connection/vectorstores/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9ac3d8-b70c-4727-b7e8-a9e709e2765f",
   "metadata": {},
   "source": [
    "#### Steps of the Retrieval and Generation phase\n",
    "* Given a question, the most relevant chunks are retrieved from the vector database using a [Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/).\n",
    "* The question and the chunks are sent to the [LLM](https://python.langchain.com/docs/modules/model_io/llms/) in a prompt. The LLM then produces the answer. You can use a [ChatModel](https://python.langchain.com/docs/modules/model_io/chat) instead of an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcb4477-64b6-4301-a0fe-3256b27b99d1",
   "metadata": {},
   "source": [
    "## Resources available from LangChain\n",
    "* [RAG Quickstart Guide](https://python.langchain.com/docs/use_cases/question_answering/quickstart).\n",
    "* [How to get the source documents used to produce a RAG answer](https://python.langchain.com/docs/use_cases/question_answering/sources).\n",
    "* [How to stream RAG answers](https://python.langchain.com/docs/use_cases/question_answering/streaming).\n",
    "* [How to add chat history to a RAG app](https://python.langchain.com/docs/use_cases/question_answering/chat_history).\n",
    "* [How to do RAG when each user has their own private data](https://python.langchain.com/docs/use_cases/question_answering/per_user).\n",
    "* [How to use Agents for RAG](https://python.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents).\n",
    "* [How to use RAG with local models](https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15883bc4-5a3e-4002-a184-5dfa9af50c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
