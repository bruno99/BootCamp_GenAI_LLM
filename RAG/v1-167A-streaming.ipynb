{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d15a62-599d-4c8f-9821-79f07ec73ff6",
   "metadata": {},
   "source": [
    "# How to stream the responses of your RAG App\n",
    "* In the context of LLM apps, streaming  refers to a continuous flow of text being delivered in the response, rather than waiting for the entire response to be completed and displayed all at once.\n",
    "* It's like reading a book where the pages are being written and shown to you one sentence at a time, without having to wait for the whole chapter to be written before you can start reading.\n",
    "* This approach allows for a more dynamic and interactive experience, as you can start receiving parts of the answer as the system generates it, making the interaction feel faster and more fluid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59939c81-6b6b-4987-84ed-8ffebc495501",
   "metadata": {},
   "source": [
    "## Dependencies and Necessary Modules\n",
    "* Same as with the project we develop for the Quickstart Guide (see notebook 159-rag-quickstart)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac6049-8508-47fc-bd49-ddf7e44d5e02",
   "metadata": {},
   "source": [
    "#### Dependencies\n",
    "* OpenAI Chat Model.\n",
    "* OpenAI Embeddings.\n",
    "* Chroma Vector Database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df588d-d5b2-404f-8f6b-2b64e67ea00e",
   "metadata": {},
   "source": [
    "#### Necessary Modules:\n",
    "* langchain\n",
    "* langchain-community\n",
    "* langchainhub\n",
    "* langchain-openai\n",
    "* chromadb\n",
    "* bs4: for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e9124f1-2235-4503-a8d9-57a1038f42db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc3f93a-0d31-4b28-93c1-6bcc3fa0369e",
   "metadata": {},
   "source": [
    "## .env file\n",
    "* OpenAI API key.\n",
    "* LangSmith credentials.\n",
    "* Our LangSmith project name: RAGstreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e74a9b-f3d9-49f0-a6e9-89a3aa1de0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de808b-c0e9-48a9-b334-666cf8482df8",
   "metadata": {},
   "source": [
    "## Open LangSmith to track the following operations\n",
    "* smith.langchain.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f8d23-2438-44dd-8347-a90323ad8344",
   "metadata": {},
   "source": [
    "## The initial RAG App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008a3e7-4a6e-4ba0-825c-d849307b780d",
   "metadata": {},
   "source": [
    "#### Required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0468b4b2-8614-40c6-a3fb-82d594e10c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "830b9e9c-524e-4672-9625-874f780caa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "bs_strainer = bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6304eb7-b608-4e1b-8974-93bb54dceaa8",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941ecb5-5cc7-4cff-85e8-50662ce78794",
   "metadata": {},
   "source": [
    "* With LCEL it’s easy to stream final outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e080f6-499a-47ab-bf9e-9794f770f0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is Task Decomposition'}\n",
      "{'context': [Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]}\n",
      "{'answer': ''}\n",
      "{'answer': 'Task'}\n",
      "{'answer': ' decomposition'}\n",
      "{'answer': ' is'}\n",
      "{'answer': ' a'}\n",
      "{'answer': ' technique'}\n",
      "{'answer': ' used'}\n",
      "{'answer': ' to'}\n",
      "{'answer': ' break'}\n",
      "{'answer': ' down'}\n",
      "{'answer': ' complex'}\n",
      "{'answer': ' tasks'}\n",
      "{'answer': ' into'}\n",
      "{'answer': ' smaller'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' simpler'}\n",
      "{'answer': ' steps'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' It'}\n",
      "{'answer': ' involves'}\n",
      "{'answer': ' transforming'}\n",
      "{'answer': ' big'}\n",
      "{'answer': ' tasks'}\n",
      "{'answer': ' into'}\n",
      "{'answer': ' multiple'}\n",
      "{'answer': ' manageable'}\n",
      "{'answer': ' tasks'}\n",
      "{'answer': ' to'}\n",
      "{'answer': ' aid'}\n",
      "{'answer': ' in'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' interpretation'}\n",
      "{'answer': ' of'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' model'}\n",
      "{'answer': \"'s\"}\n",
      "{'answer': ' thinking'}\n",
      "{'answer': ' process'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' Different'}\n",
      "{'answer': ' methods'}\n",
      "{'answer': ' like'}\n",
      "{'answer': ' Chain'}\n",
      "{'answer': ' of'}\n",
      "{'answer': ' Thought'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' Tree'}\n",
      "{'answer': ' of'}\n",
      "{'answer': ' Thoughts'}\n",
      "{'answer': ' can'}\n",
      "{'answer': ' be'}\n",
      "{'answer': ' used'}\n",
      "{'answer': ' to'}\n",
      "{'answer': ' decom'}\n",
      "{'answer': 'pose'}\n",
      "{'answer': ' tasks'}\n",
      "{'answer': ' effectively'}\n",
      "{'answer': '.'}\n",
      "{'answer': ''}\n"
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain_with_source.stream(\"What is Task Decomposition\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f63b40-c780-4a96-967c-b48996efc380",
   "metadata": {},
   "source": [
    "* We can add some logic to compile our stream as it’s being returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2413171e-f533-4897-9335-f42a197a1bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "question: What is Task Decomposition\n",
      "\n",
      "context: [Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]\n",
      "\n",
      "answer: Task decomposition is a technique that breaks down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks to aid in the interpretation of the model's thinking process. Different methods, such as Chain of Thought and Tree of Thoughts, can be used to decompose tasks effectively."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is Task Decomposition',\n",
       " 'context': [Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       "  Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       "  Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       "  Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})],\n",
       " 'answer': \"Task decomposition is a technique that breaks down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks to aid in the interpretation of the model's thinking process. Different methods, such as Chain of Thought and Tree of Thoughts, can be used to decompose tasks effectively.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = {}\n",
    "curr_key = None\n",
    "for chunk in rag_chain_with_source.stream(\"What is Task Decomposition\"):\n",
    "    for key in chunk:\n",
    "        if key not in output:\n",
    "            output[key] = chunk[key]\n",
    "        else:\n",
    "            output[key] += chunk[key]\n",
    "        if key != curr_key:\n",
    "            print(f\"\\n\\n{key}: {chunk[key]}\", end=\"\", flush=True)\n",
    "        else:\n",
    "            print(chunk[key], end=\"\", flush=True)\n",
    "        curr_key = key\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2360dffe-5891-42ea-9886-0b43860fbb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
