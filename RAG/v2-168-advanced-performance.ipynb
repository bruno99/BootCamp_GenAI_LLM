{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe",
   "metadata": {},
   "source": [
    "# Advanced techniques to improve RAG Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706cf9f9-399c-44f6-b820-27642431314a",
   "metadata": {},
   "source": [
    "# Basic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f53325-72b8-4aac-9ded-06047efe0789",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae0ab7-d43b-43e0-8b99-6122a636fe0c",
   "metadata": {},
   "source": [
    "Documentation for RAG with LangChain: [RAG quickstart](https://python.langchain.com/docs/use_cases/question_answering/quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218daf25-3d66-4730-a296-bf866cdf2eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e543e142-378e-4208-a8a6-2b194b1a30ae",
   "metadata": {},
   "source": [
    "#### Recommended: create new virtualenv\n",
    "* pyenv virtualenv 3.11.4 your_venv_name\n",
    "* pyenv activate your_venv_name\n",
    "* pip install jupyterlab\n",
    "* jupyter lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945ac54-d78b-4de1-831e-2d730d486260",
   "metadata": {},
   "source": [
    "#### Install Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a88555a-53a5-4ab8-ba3d-e6dd3a26c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ec3656-01c6-42e5-aac1-ce544a192eed",
   "metadata": {},
   "source": [
    "#### Recommended: activate LangSmith from the beginning\n",
    "* LangSmith credential in the .env file:\n",
    "    * LANGCHAIN_TRACING_V2=true\n",
    "    * LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "    * LANGCHAIN_API_KEY=your_key\n",
    "    * LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fdc7cfa3-7cea-4460-b8f7-17c079240eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1387ced-eeae-4581-8731-f66b5af2de1b",
   "metadata": {},
   "source": [
    "#### Load the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06208df8-54c7-4ed2-b4ee-b28cc999120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f83d28-e100-4d51-ab81-894ee18d4158",
   "metadata": {},
   "source": [
    "## Phase 1: Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56d6a5-0c6b-4ef1-8c81-023b6ffb485b",
   "metadata": {},
   "source": [
    "#### Load the private document. In this case, we will use a web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e108f6-92be-452a-9eb0-2af6613c7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document, in this case from web page\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0208a-1bd6-4511-bb27-942a0f85de3d",
   "metadata": {},
   "source": [
    "#### Split the document in small chunks of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "662ef6db-f3f6-46ac-8494-f37481c462fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split document in small chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72035fc9-4b29-4f1f-b885-419c0736dac8",
   "metadata": {},
   "source": [
    "#### Convert the chunks of text in numbers (embeddings) and load them into a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98a80bb7-38a3-47aa-8b6a-0748d1594675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert small chunks in numbers (embeddings) and store in vector database\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a50ef0-9cdc-4e52-9b47-fc96ac4fa6de",
   "metadata": {},
   "source": [
    "## Phase 2: Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7460df-2f0d-4372-b29e-c9fa0303e3c5",
   "metadata": {},
   "source": [
    "#### Set the retriever\n",
    "A \"retriever\" refers to a component or object that is used to retrieve (or search for) specific information from a database or a collection of data. The retriever is designed to work with a vector database that stores data in the form of embeddings.\n",
    "\n",
    "Embeddings are numerical representations of data (in this case, text data split into smaller chunks) that capture the semantic meaning of the original content. These embeddings are created using models like the ones provided by OpenAI (for example, through `OpenAIEmbeddings`), which convert textual content into high-dimensional vectors that numerically represent the semantic content of the text.\n",
    "\n",
    "The vector database, in this context represented by `Chroma`, is a specialized database that stores these embeddings. The primary function of this database is to enable efficient similarity searches. That is, given a query in the form of an embedding, the database can quickly find and return the most similar embeddings (and by extension, the corresponding chunks of text or documents) stored within it.\n",
    "\n",
    "When the code sets the `retriever` as `vectorstore.as_retriever()`, it essentially initializes a retriever object with the capability to perform these similarity-based searches within the `Chroma` vector database. This means that the retriever can be used to find the most relevant pieces of information (text chunks, in this scenario) based on a query. This is particularly useful in applications that needs to retrieve information based on semantic similarity rather than exact keyword matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a0677e-dc3f-4cc5-a8e5-2ff16b9181a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4a71f8-4953-4972-a94c-5dc77dad7e38",
   "metadata": {},
   "source": [
    "#### Set the prompt\n",
    "* In this case, we will get a pre-defined prompt from the LangSmith Prompt Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70d84d70-47a9-4f5f-beb8-4023da092d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54736ff4-7223-4e7f-a7db-14263d8fdb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's print the prompt we have selected from the LangSmith Prompt Hub\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1192e9b5-4d5f-4467-887e-54136eda27ba",
   "metadata": {},
   "source": [
    "* Option 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eae49180-3676-429a-98b0-abe9d39bc727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# template = \"\"\"Answer the question based only on the following context:\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509e63b-79a4-452b-8e38-6c846755c1c5",
   "metadata": {},
   "source": [
    "#### Determine which Foundation LLM we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcca114a-218f-4dd9-a09d-a2f9d924782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6905ebec-c478-40ea-a429-b2108a8390a9",
   "metadata": {},
   "source": [
    "#### Define a pre-processing to better format the document\n",
    "* This step is optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e050b428-2a31-4465-b7ef-d6f0249420d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c3293-7d6e-4e5d-9dea-0cf42392bc77",
   "metadata": {},
   "source": [
    "#### Define the RAG chain\n",
    "Pay attention to this details:\n",
    "* See how we use the retriever as context\n",
    "* See how we use the pre-processing function\n",
    "* See how we use the StrOutputParser to format the answer as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebde2fc0-669b-4b52-9999-b047f8a0613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6f6e7-58f8-444f-9c95-c2d7448666f8",
   "metadata": {},
   "source": [
    "## Phase 3: Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4858ab5-d21a-4fde-9cb3-613948f9144b",
   "metadata": {},
   "source": [
    "#### We can start asking questions to our RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98070313-0c2f-4ba6-ae3e-79e2418ce4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a technique used to break down complex tasks into smaller and simpler steps. This approach helps agents to plan and execute tasks more efficiently by dividing them into manageable components. Task decomposition can be achieved through various methods, such as using prompting techniques, task-specific instructions, or human inputs.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a752c6-270a-4e43-ab00-6cce00208380",
   "metadata": {},
   "source": [
    "# Advanced techniques to improve RAG Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f59f5e1-1086-4e72-9fa3-3454f2f055fa",
   "metadata": {},
   "source": [
    "# 1. Query translation \n",
    "* Improving a question before it might lead to a low quality anwser from our RAG application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa9dd91-d0ac-4b7b-9cd4-db067c8d45c8",
   "metadata": {},
   "source": [
    "#### Alternative ways to transform the initial question:\n",
    "* Convert the question in multiple similar questions\n",
    "    * Multi-Query\n",
    "    * RAG Fusion\n",
    "* Convert the question in (progressive or not) subquestions\n",
    "* Step-back questions (step-back prompting): go one step back and make the questions that precede or originate the user's question. Example:\n",
    "    * User question: where were Mozart born?\n",
    "    * Step-back question: who was Mozart? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f2bcc-a756-4b93-8967-758bfebede4d",
   "metadata": {},
   "source": [
    "## Query translation technique #1: Multi-Query\n",
    "* Convert the question into several similar questions\n",
    "* Get the RAG answer for each of them\n",
    "* Analyze all the answers and produce a final one "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda6dc5-3c60-4525-87d2-509df09cc6ae",
   "metadata": {},
   "source": [
    "#### Create a prompt template to create 5 similar questions to the question provided by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81ba73ef-4026-4580-9d3f-3f2b91d560e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Query: Different Perspectives\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aede4e9-f252-4666-91bc-e2cc6812be19",
   "metadata": {},
   "source": [
    "#### Define a chain to generate the 5 similar questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4c9bb84-a5dd-48ae-8e68-474b491c3418",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0d933-7c3d-4d3b-a34a-5bb3a4a95dc3",
   "metadata": {},
   "source": [
    "#### Get the RAG answer for each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e307a11-1d7d-446b-9cf1-cd28fed92e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliocolomer/.pyenv/versions/3.11.4/envs/venv032924/lib/python3.11/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9657c9b-fc7d-41ae-9983-a2822e95ddcb",
   "metadata": {},
   "source": [
    "#### Analyze all the answers and produce a final one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c778a7c0-103e-4588-9f8c-c98a6353f947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. This process allows the agent to transform big tasks into multiple manageable tasks and shed light on the interpretation of the model's thinking process.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f77bfe9-fc31-4e8e-aa0f-a25ebf234e71",
   "metadata": {},
   "source": [
    "## Query translation technique #2: RAG Fusion\n",
    "* Same as Multi-Query, but ranking the resulting questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c30b9a-860d-4ded-89ff-65155e3b4845",
   "metadata": {},
   "source": [
    "#### Same as before: create a prompt template to create 4 similar questions to the question provided by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ba3565a-935a-416b-affe-4a0fd21afc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a39789-4695-4f49-8450-f1db7f083bec",
   "metadata": {},
   "source": [
    "#### Same as before: define a chain to generate the 4 similar questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d77b333-ad80-432e-88ae-fe402f996dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3791d4a-20e0-44df-ba8a-2a0b2e7823d8",
   "metadata": {},
   "source": [
    "The following code demonstrates an implementation of the Reciprocal Rank Fusion (RRF) algorithm, leveraging functions from the `langchain.load` module for serialization and deserialization of documents. The RRF algorithm is used for combining the rankings of documents from multiple sources (or search results) into a single, consolidated ranking. The purpose is to enhance the quality of the search results by leveraging the strengths of multiple ranking systems. \n",
    "\n",
    "Here's a breakdown of the code:\n",
    "\n",
    "#### Function `reciprocal_rank_fusion`\n",
    "- **Parameters**:\n",
    "  - `results`: A list of lists, where each inner list contains documents ranked according to their relevance from different sources or queries.\n",
    "  - `k`: An optional parameter used in the RRF formula to control the influence of rank on the score. The default value is 60, which is a common choice in RRF implementations.\n",
    "\n",
    "- **Process**:\n",
    "  1. **Initialization**: A dictionary named `fused_scores` is created to store the cumulative score of each unique document across all rankings.\n",
    "  2. **Ranking and Scoring**:\n",
    "     - The code iterates over each list of ranked documents (`results`).\n",
    "     - For each document in these lists, it calculates a score based on its rank using the RRF formula: `1 / (rank + k)`, where `rank` is the document's position in the list (starting from 0).\n",
    "     - The document is serialized into a string (`doc_str`) using `dumps`, and this string serves as a key in the `fused_scores` dictionary. This approach assumes that documents can be uniquely identified after serialization.\n",
    "     - If the document is new to `fused_scores`, it's initialized with a score of 0. Then, the RRF score is added to the document's existing score in the dictionary.\n",
    "  3. **Sorting**: After processing all lists, the documents are sorted in descending order based on their cumulative scores in `fused_scores`.\n",
    "\n",
    "- **Output**: The function returns the reranked results as a list of tuples, with each tuple containing the deserialized document and its fused score, sorted from highest to lowest score.\n",
    "\n",
    "### RRF Algorithm in Simple Terms\n",
    "The RRF algorithm works under the principle that documents ranked highly across multiple searches are likely more relevant than those ranked highly in just one. By inversely weighting the ranks (giving higher value to lower ranks), it ensures that documents consistently ranked well across different lists are prioritized in the final consolidated list.\n",
    "\n",
    "### Usage Example\n",
    "- **Chain of Operations**: The code snippet demonstrates how `reciprocal_rank_fusion` could be integrated into a larger pipeline (`retrieval_chain_rag_fusion`) that generates queries, retrieves documents based on those queries, and then applies the RRF algorithm to the results.\n",
    "- **Invocation**: The `invoke` method is used to execute this pipeline with a specific `question`, and `len(docs)` gives the total number of documents after reranking.\n",
    "\n",
    "This example showcases a practical application of RRF in enhancing search or retrieval systems by aggregating and refining results from multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4de1e7e1-9aef-4535-a448-0ea0fc852468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791c7d7-e751-4d4b-aef0-d381273059a0",
   "metadata": {},
   "source": [
    "## Query translation technique #3: Decomposition\n",
    "* Decompose de question into progressive sub-questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e61183-1bb3-4f64-910d-630c45759589",
   "metadata": {},
   "source": [
    "#### Create a prompt template to create 3 subquestions from the question provided by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8acdf02d-6c96-4907-8392-866c22d2b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f4338b9-d95d-4a94-886d-f72e0979de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "\n",
    "# These are the subquestions:\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e3121ac-1d15-47a0-b9e5-bf0cd10eda0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is LLM technology and how does it work in autonomous agent systems?',\n",
       " '2. What are the specific components that make up an autonomous agent system powered by LLM?',\n",
       " '3. How do the main components of an LLM-powered autonomous agent system interact with each other to achieve autonomy and intelligence?']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the subquestions:\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c8fe9-f6c4-46f7-b545-6c4e3fca4bcb",
   "metadata": {},
   "source": [
    "#### Option 1: answer each of the subquestions using the response to the previous subquestion as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02c3af77-85af-41f5-aa10-0afbb9f5c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f7fab6c-e271-4711-b6bb-35d7bcb0f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "469351b0-e861-4d14-9585-7eebbe2cf8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The main components of an LLM-powered autonomous agent system interact with each other in a coordinated manner to achieve autonomy and intelligence. \\n\\n1. Planning: The agent utilizes planning to break down complex tasks into smaller subgoals, enabling efficient handling of tasks. This component interacts with subgoal decomposition to ensure that tasks are divided into manageable steps for better performance on complex tasks.\\n\\n2. Subgoal Decomposition: Tasks are decomposed into smaller and simpler steps, which enhances the model's performance on complex tasks. This component works in conjunction with planning to ensure that the agent can efficiently handle tasks by breaking them down into manageable subgoals.\\n\\n3. Reflection and Refinement: The agent engages in self-criticism and self-reflection over past actions, learning from mistakes and refining its approach for future tasks. This component interacts with planning and subgoal decomposition to continuously improve the agent's performance by reflecting on past actions and refining strategies for future tasks.\\n\\n4. Natural Language Interface: The agent system relies on natural language as an interface between LLMs and external components like memory and tools. While the reliability of model outputs may sometimes be questionable, this component plays a crucial role in facilitating communication between the agent's brain (LLM) and external components, enhancing the overall autonomy and intelligence of the system.\\n\\nOverall, the interaction between these components allows an LLM-powered autonomous agent system to effectively plan, decompose tasks, reflect on past actions, and communicate with external components, ultimately leading to autonomy and intelligence in handling complex tasks and improving performance over time.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ebb22b-7a35-47ee-a76d-4c5d9c3c0bb3",
   "metadata": {},
   "source": [
    "#### Option 2: answer each of the subquestions individually (not using the response to the previous subquestion as context).\n",
    "* Useful when the subquestions are not dependent from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ecd205f-efad-4d65-a9b9-c8779fc1e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer each sub-question individually \n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# RAG prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n",
    "                                                                \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a7847d2-b729-4435-92e7-d80fe014f435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main components of an LLM-powered autonomous agent system include the Large Language Model (LLM) as the core controller, planning for task decomposition, reflection for self-criticism and learning from mistakes, and refinement for improving future actions. Additionally, the reliability of the natural language interface is crucial for communication between the LLM and external components. These components work together to enable autonomous functionality, allowing the agent to break down complex tasks, learn from errors, and continuously improve its performance.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":context,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f64ba7-3a7f-4eac-b429-985c200ce2cf",
   "metadata": {},
   "source": [
    "## Query translation technique #4: Step-back questions (also called step-back prompting)\n",
    "* Step-back questions (step-back prompting): go one step back and make the questions that precede or originate the user's question. Example:\n",
    "    * User question: where were Mozart born?\n",
    "    * Step-back question: who was Mozart? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac6c2464-04e7-49ec-a26f-e00f36be3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d8d4f28-4da4-4fb1-8e21-9f3d34c940a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the process of breaking down tasks for LLM agents?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1281ec03-54b4-4bf6-8d05-c955241799b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents refers to the process of breaking down complex tasks into smaller, more manageable subgoals. This approach enables LLM-powered autonomous agents to efficiently handle intricate tasks by dividing them into simpler steps. By decomposing tasks, the agent can effectively plan and execute each subgoal, leading to the successful completion of the overall task.\\n\\nIn the context of LLM-powered autonomous agent systems, task decomposition plays a crucial role in enhancing the performance and problem-solving capabilities of the agent. By breaking down large tasks into smaller components, the agent can focus on addressing one subgoal at a time, leading to a more systematic and organized approach to task completion.\\n\\nOne common technique used for task decomposition in LLM agents is the Chain of Thought (CoT) method. CoT prompts the model to \"think step by step,\" encouraging it to decompose complex tasks into smaller and simpler steps. This approach leverages the model\\'s test-time computation to effectively break down hard tasks into manageable components. Additionally, the Tree of Thoughts extends the CoT method by exploring multiple reasoning possibilities at each step, creating a tree structure of thought steps and generating multiple thoughts per step.\\n\\nTask decomposition can be facilitated through various means, such as simple prompting by the LLM, task-specific instructions, or human inputs. By decomposing tasks into smaller subgoals, LLM agents can efficiently navigate through complex tasks, improve their problem-solving abilities, and ultimately achieve better results.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Response prompt \n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627eef9a-c857-4bcc-b19f-ffe1c469b471",
   "metadata": {},
   "source": [
    "## Query Translation Technique #5: The HyDE Technique\n",
    "* Given the user's question, create a fake document that would answer it properly.\n",
    "* Then go to the vector database and find documents similar to the fake document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e15803a2-5e3f-4932-a54b-212d05469699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a fundamental concept in the field of reinforcement learning and artificial intelligence, particularly for Large Language Models (LLMs) agents. Task decomposition refers to the process of breaking down a complex task into smaller, more manageable sub-tasks that can be solved independently or sequentially. \\n\\nIn the context of LLM agents, task decomposition plays a crucial role in improving the efficiency and effectiveness of the learning process. By decomposing a complex task into smaller sub-tasks, LLM agents can focus on solving each sub-task individually, which can lead to faster learning and better performance overall.\\n\\nFurthermore, task decomposition allows LLM agents to leverage their language understanding capabilities to interpret and generate instructions for each sub-task. This can help the agent to better understand the overall task structure and requirements, leading to more accurate and effective decision-making.\\n\\nOverall, task decomposition for LLM agents is a powerful technique that can enhance the learning process and improve the performance of these agents in a wide range of tasks and domains. By breaking down complex tasks into smaller, more manageable sub-tasks, LLM agents can achieve better results and make more informed decisions.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HyDE document generation\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0501fa6-6b20-406d-aee4-db346295ac05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retireved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "retireved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "546e95d4-2276-447f-a6ea-53e9f35f5e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. This process allows the agent to utilize more test-time computation to decompose hard tasks into smaller and simpler steps, ultimately improving the quality of final results.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f8b6f1-462c-4e42-ad42-eebf6fb1e3c9",
   "metadata": {},
   "source": [
    "# Routing\n",
    "Purpose: To direct the question to the appropriate data source (such as a vector database, relational database, or graph database) or the appropriate prompt or any other routing option.\n",
    "\n",
    "Types of Routing:\n",
    "* Logical Routing: Uses the LLM's knowledge of data sources to decide the best destination for a query.\n",
    "* Semantic Routing: Involves embedding the question and prompts, calculating similarity, and selecting the prompt with the highest similarity for routing. For example: based on the question, use the Math prompt or the Physics prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb3a86-8b1d-46ad-964d-bf48768cf3f5",
   "metadata": {},
   "source": [
    "#### Logical Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac3aa8f3-7d35-40bf-8bd1-fae570e133aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliocolomer/.pyenv/versions/3.11.4/envs/venv032924/lib/python3.11/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99e24506-9f17-4983-93be-fb551d8e98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b1cb1c8-d973-441a-b5fa-577c31f7604c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='python_docs')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a691fef-02b1-4593-b30a-78930801f766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python_docs'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8aed77ca-cc8b-4853-b95a-9e3d22cad237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for python_docs\"\n",
    "    elif \"js_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for js_docs\"\n",
    "    else:\n",
    "        ### Logic here \n",
    "        return \"golang_docs\"\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8779d80d-2f31-4103-9019-d0740a2769d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chain for python_docs'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ea7aa9-8652-4e20-853f-90a9fcbd9037",
   "metadata": {},
   "source": [
    "#### Semantic Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4e5160f-efc6-4bbd-a3b5-7447481c4e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n",
      "A black hole is a region in space where the gravitational pull is so strong that nothing, not even light, can escape from it. It is formed when a massive star collapses in on itself, creating a singularity with infinite density at its center. Black holes have a boundary called the event horizon, beyond which nothing can escape. They are mysterious and fascinating objects in the universe that have many interesting properties and effects on their surroundings.\n"
     ]
    }
   ],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Two prompts\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "# Embed prompts\n",
    "embeddings = OpenAIEmbeddings()\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "# Route question to prompt \n",
    "def prompt_router(input):\n",
    "    # Embed question\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    # Compute similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    # Chosen prompt \n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What's a black hole\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9d4ef-efce-4885-9f54-262fb8e5afdd",
   "metadata": {},
   "source": [
    "# Query structuring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67554be7-8609-4a2e-af6e-7c4f68a7774e",
   "metadata": {},
   "source": [
    "Transform the query from natural language to some query syntax.\n",
    "\n",
    "In the example below, we transform the query from natural language to the query syntax according to the schema we have defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "abd7f4b9-be4e-4b9f-b05c-c2fa97b2f7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'pbAd8O1Lvm4',\n",
       " 'title': 'Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'description': 'Unknown',\n",
       " 'view_count': 13798,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/pbAd8O1Lvm4/hq720.jpg',\n",
       " 'publish_date': '2024-02-07 00:00:00',\n",
       " 'length': 1058,\n",
       " 'author': 'LangChain'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "docs = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=pbAd8O1Lvm4\", add_video_info=True\n",
    ").load()\n",
    "\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06a21dbe-a1d6-4ea3-b90c-f0a58098aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Literal, Optional, Tuple\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class TutorialSearch(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    content_search: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    title_search: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Alternate version of the content search query to apply to video titles. \"\n",
    "            \"Should be succinct and only include key words that could be in a video \"\n",
    "            \"title.\"\n",
    "        ),\n",
    "    )\n",
    "    min_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum view count filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum view count filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    earliest_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Earliest publish date filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    latest_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Latest publish date filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    min_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum video length in seconds, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum video length in seconds, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        for field in self.__fields__:\n",
    "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
    "                self.__fields__[field], \"default\", None\n",
    "            ):\n",
    "                print(f\"{field}: {getattr(self, field)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64b3cb7a-52f0-48c5-9774-b70cdf5ebe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a database query optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(TutorialSearch)\n",
    "query_analyzer = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e07610b1-560f-411d-ad7b-628ff7850344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d93e6941-99ad-4ebe-afe5-aa14c8832b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: rag from scratch\n",
      "title_search: rag from scratch\n"
     ]
    }
   ],
   "source": [
    "query_analyzer.invoke({\"question\": \"rag from scratch\"}).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ff61e88-91af-43e2-a1a4-5d2730e33b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: chat langchain\n",
      "title_search: chat langchain\n",
      "earliest_publish_date: 2023-01-01\n",
      "latest_publish_date: 2024-01-01\n"
     ]
    }
   ],
   "source": [
    "query_analyzer.invoke(\n",
    "    {\"question\": \"videos on chat langchain published in 2023\"}\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "657a40cf-004a-4470-9da7-3199f7d11b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: multi-modal models agent\n",
      "title_search: multi-modal models agent\n",
      "max_length_sec: 300\n"
     ]
    }
   ],
   "source": [
    "query_analyzer.invoke(\n",
    "    {\n",
    "        \"question\": \"how to use multi-modal models in an agent, only videos under 5 minutes\"\n",
    "    }\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74344f8-483f-4d71-8f0f-05194d132416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
